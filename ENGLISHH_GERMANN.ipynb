{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-27T13:52:00.166726Z",
     "iopub.status.busy": "2023-06-27T13:52:00.166240Z",
     "iopub.status.idle": "2023-06-27T13:52:00.171899Z",
     "shell.execute_reply": "2023-06-27T13:52:00.170872Z",
     "shell.execute_reply.started": "2023-06-27T13:52:00.166687Z"
    }
   },
   "source": [
    "# Machine Translation:    ENGLISH TO GERMAN\n",
    "# (Encoder-Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Add the image -->\n",
    "![Alt Text](encode_decode.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and preprocessing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T06:10:31.872221Z",
     "iopub.status.busy": "2023-06-30T06:10:31.871619Z",
     "iopub.status.idle": "2023-06-30T06:10:32.207549Z",
     "shell.execute_reply": "2023-06-30T06:10:32.206606Z",
     "shell.execute_reply.started": "2023-06-30T06:10:31.872187Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28/3543679006.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  english_sentences = data['ENGLISH'].str.lower().str.replace('[^\\w\\s]', '').tolist()\n",
      "/tmp/ipykernel_28/3543679006.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  german_sentences = data['GERMAN'].str.lower().str.replace('[^\\w\\s]', '').apply(lambda x: '<start> ' + x + ' <end>').tolist()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('/kaggle/input/hey-buddy/GERMAN_ENGLISH_TRANSLATION.csv')\n",
    "data = data.drop_duplicates(subset=['ENGLISH'])\n",
    "data = data.head(20000)\n",
    "\n",
    "english_sentences = data['ENGLISH'].str.lower().str.replace('[^\\w\\s]', '').tolist()\n",
    "german_sentences = data['GERMAN'].str.lower().str.replace('[^\\w\\s]', '').apply(lambda x: '<start> ' + x + ' <end>').tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T06:10:32.209602Z",
     "iopub.status.busy": "2023-06-30T06:10:32.209013Z",
     "iopub.status.idle": "2023-06-30T06:10:32.226835Z",
     "shell.execute_reply": "2023-06-30T06:10:32.225822Z",
     "shell.execute_reply.started": "2023-06-30T06:10:32.209569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 ENGLISH      GERMAN\n",
      "0           0      hi       hallo\n",
      "2           2     run        lauf\n",
      "3           3     wow  potzdonner\n",
      "5           5    fire       feuer\n",
      "6           6    help       hilfe\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T06:10:32.229741Z",
     "iopub.status.busy": "2023-06-30T06:10:32.229408Z",
     "iopub.status.idle": "2023-06-30T06:10:32.237658Z",
     "shell.execute_reply": "2023-06-30T06:10:32.236722Z",
     "shell.execute_reply.started": "2023-06-30T06:10:32.229710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> hallo <end>', '<start> lauf <end>', '<start> potzdonner <end>', '<start> feuer <end>', '<start> hilfe <end>']\n"
     ]
    }
   ],
   "source": [
    "print(german_sentences[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T06:10:32.239655Z",
     "iopub.status.busy": "2023-06-30T06:10:32.239176Z",
     "iopub.status.idle": "2023-06-30T06:10:32.994559Z",
     "shell.execute_reply": "2023-06-30T06:10:32.993590Z",
     "shell.execute_reply.started": "2023-06-30T06:10:32.239624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 13\n"
     ]
    }
   ],
   "source": [
    "# Integer encode sentences\n",
    "eng_token = Tokenizer(filters='')\n",
    "eng_token.fit_on_texts(english_sentences)\n",
    "eng_token_ind = eng_token.texts_to_sequences(english_sentences)\n",
    "\n",
    "ger_token = Tokenizer(filters='')\n",
    "ger_token.fit_on_texts(german_sentences)\n",
    "ger_token_ind= ger_token.texts_to_sequences(german_sentences)\n",
    "\n",
    "# Pad encoded sentences\n",
    "max_encoder_seq_length = max([len(seq) for seq in eng_token_ind])\n",
    "max_decoder_seq_length = max([len(seq) for seq in ger_token_ind])\n",
    "print(max_encoder_seq_length,max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Encoder (input data) and decoder (input,target) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T06:10:32.997280Z",
     "iopub.status.busy": "2023-06-30T06:10:32.996392Z",
     "iopub.status.idle": "2023-06-30T06:10:34.059813Z",
     "shell.execute_reply": "2023-06-30T06:10:34.058798Z",
     "shell.execute_reply.started": "2023-06-30T06:10:32.997246Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_input_data = pad_sequences(eng_token_ind, maxlen=max_encoder_seq_length, padding='post')\n",
    "decoder_input_data = pad_sequences(ger_token_ind, maxlen=max_decoder_seq_length, padding='post')\n",
    "\n",
    "# target data for the decoder\n",
    "decoder_target_data = []\n",
    "for seq in ger_token_ind:\n",
    "    decoder_target_data.append(seq[1:])\n",
    "decoder_target_data = pad_sequences(decoder_target_data, maxlen=max_decoder_seq_length, padding='post')\n",
    "\n",
    "num_decoder_tokens = len(ger_token.word_index) + 1\n",
    "decoder_output = np.zeros((len(ger_token_ind), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "for i, seq in enumerate(decoder_target_data):\n",
    "    for t, token in enumerate(seq):\n",
    "        decoder_output[i, t, token] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture- LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T06:10:34.062147Z",
     "iopub.status.busy": "2023-06-30T06:10:34.061780Z",
     "iopub.status.idle": "2023-06-30T06:10:37.482133Z",
     "shell.execute_reply": "2023-06-30T06:10:37.481134Z",
     "shell.execute_reply.started": "2023-06-30T06:10:34.062115Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "latent_dim = 256\n",
    "num_encoder_tokens = len(eng_token.word_index) + 1\n",
    "\n",
    "eng_embedding_layer = Embedding(num_encoder_tokens, latent_dim)\n",
    "ger_embedding_layer = Embedding(num_decoder_tokens, latent_dim)\n",
    "\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = eng_embedding_layer(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = ger_embedding_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T06:10:38.945724Z",
     "iopub.status.busy": "2023-06-30T06:10:38.945330Z",
     "iopub.status.idle": "2023-06-30T06:23:28.014444Z",
     "shell.execute_reply": "2023-06-30T06:23:28.013277Z",
     "shell.execute_reply.started": "2023-06-30T06:10:38.945694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "250/250 [==============================] - 36s 112ms/step - loss: 2.4325 - accuracy: 0.6773 - val_loss: 2.2305 - val_accuracy: 0.6750\n",
      "Epoch 2/60\n",
      "250/250 [==============================] - 13s 50ms/step - loss: 1.7670 - accuracy: 0.7303 - val_loss: 2.1167 - val_accuracy: 0.6826\n",
      "Epoch 3/60\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 1.6095 - accuracy: 0.7475 - val_loss: 1.9734 - val_accuracy: 0.7143\n",
      "Epoch 4/60\n",
      "250/250 [==============================] - 13s 52ms/step - loss: 1.4438 - accuracy: 0.7709 - val_loss: 1.8405 - val_accuracy: 0.7317\n",
      "Epoch 5/60\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 1.2906 - accuracy: 0.7913 - val_loss: 1.7410 - val_accuracy: 0.7482\n",
      "Epoch 6/60\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 1.1592 - accuracy: 0.8067 - val_loss: 1.6627 - val_accuracy: 0.7595\n",
      "Epoch 7/60\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 1.0489 - accuracy: 0.8194 - val_loss: 1.5983 - val_accuracy: 0.7660\n",
      "Epoch 8/60\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.9498 - accuracy: 0.8308 - val_loss: 1.5485 - val_accuracy: 0.7729\n",
      "Epoch 9/60\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 0.8572 - accuracy: 0.8416 - val_loss: 1.5138 - val_accuracy: 0.7784\n",
      "Epoch 10/60\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.7717 - accuracy: 0.8518 - val_loss: 1.4852 - val_accuracy: 0.7831\n",
      "Epoch 11/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.6926 - accuracy: 0.8622 - val_loss: 1.4530 - val_accuracy: 0.7874\n",
      "Epoch 12/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.6185 - accuracy: 0.8734 - val_loss: 1.4297 - val_accuracy: 0.7921\n",
      "Epoch 13/60\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.5494 - accuracy: 0.8846 - val_loss: 1.4196 - val_accuracy: 0.7946\n",
      "Epoch 14/60\n",
      "250/250 [==============================] - 13s 50ms/step - loss: 0.4862 - accuracy: 0.8961 - val_loss: 1.4135 - val_accuracy: 0.7980\n",
      "Epoch 15/60\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.4278 - accuracy: 0.9083 - val_loss: 1.4041 - val_accuracy: 0.7996\n",
      "Epoch 16/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.3744 - accuracy: 0.9198 - val_loss: 1.3971 - val_accuracy: 0.8014\n",
      "Epoch 17/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.3267 - accuracy: 0.9311 - val_loss: 1.4019 - val_accuracy: 0.8036\n",
      "Epoch 18/60\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.2838 - accuracy: 0.9408 - val_loss: 1.4059 - val_accuracy: 0.8062\n",
      "Epoch 19/60\n",
      "250/250 [==============================] - 13s 51ms/step - loss: 0.2459 - accuracy: 0.9501 - val_loss: 1.4106 - val_accuracy: 0.8077\n",
      "Epoch 20/60\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.2123 - accuracy: 0.9581 - val_loss: 1.4192 - val_accuracy: 0.8096\n",
      "Epoch 21/60\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.1822 - accuracy: 0.9656 - val_loss: 1.4227 - val_accuracy: 0.8096\n",
      "Epoch 22/60\n",
      "250/250 [==============================] - 13s 50ms/step - loss: 0.1566 - accuracy: 0.9713 - val_loss: 1.4388 - val_accuracy: 0.8101\n",
      "Epoch 23/60\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.1339 - accuracy: 0.9767 - val_loss: 1.4452 - val_accuracy: 0.8104\n",
      "Epoch 24/60\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 0.1147 - accuracy: 0.9805 - val_loss: 1.4676 - val_accuracy: 0.8102\n",
      "Epoch 25/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0986 - accuracy: 0.9840 - val_loss: 1.4709 - val_accuracy: 0.8115\n",
      "Epoch 26/60\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0837 - accuracy: 0.9873 - val_loss: 1.4846 - val_accuracy: 0.8120\n",
      "Epoch 27/60\n",
      "250/250 [==============================] - 13s 51ms/step - loss: 0.0716 - accuracy: 0.9897 - val_loss: 1.5073 - val_accuracy: 0.8125\n",
      "Epoch 28/60\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0608 - accuracy: 0.9918 - val_loss: 1.5230 - val_accuracy: 0.8118\n",
      "Epoch 29/60\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0515 - accuracy: 0.9935 - val_loss: 1.5281 - val_accuracy: 0.8137\n",
      "Epoch 30/60\n",
      "250/250 [==============================] - 13s 50ms/step - loss: 0.0437 - accuracy: 0.9950 - val_loss: 1.5483 - val_accuracy: 0.8127\n",
      "Epoch 31/60\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0378 - accuracy: 0.9958 - val_loss: 1.5557 - val_accuracy: 0.8125\n",
      "Epoch 32/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0334 - accuracy: 0.9965 - val_loss: 1.5761 - val_accuracy: 0.8130\n",
      "Epoch 33/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0276 - accuracy: 0.9976 - val_loss: 1.5800 - val_accuracy: 0.8138\n",
      "Epoch 34/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0237 - accuracy: 0.9981 - val_loss: 1.6075 - val_accuracy: 0.8137\n",
      "Epoch 35/60\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0205 - accuracy: 0.9985 - val_loss: 1.6161 - val_accuracy: 0.8127\n",
      "Epoch 36/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0170 - accuracy: 0.9990 - val_loss: 1.6249 - val_accuracy: 0.8139\n",
      "Epoch 37/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0140 - accuracy: 0.9993 - val_loss: 1.6451 - val_accuracy: 0.8142\n",
      "Epoch 38/60\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0127 - accuracy: 0.9994 - val_loss: 1.6596 - val_accuracy: 0.8134\n",
      "Epoch 39/60\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0115 - accuracy: 0.9995 - val_loss: 1.6724 - val_accuracy: 0.8130\n",
      "Epoch 40/60\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0105 - accuracy: 0.9995 - val_loss: 1.6794 - val_accuracy: 0.8136\n",
      "Epoch 41/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0144 - accuracy: 0.9986 - val_loss: 1.7091 - val_accuracy: 0.8108\n",
      "Epoch 42/60\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0266 - accuracy: 0.9955 - val_loss: 1.7163 - val_accuracy: 0.8108\n",
      "Epoch 43/60\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 0.0244 - accuracy: 0.9959 - val_loss: 1.6964 - val_accuracy: 0.8120\n",
      "Epoch 44/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0123 - accuracy: 0.9988 - val_loss: 1.7175 - val_accuracy: 0.8154\n",
      "Epoch 45/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0062 - accuracy: 0.9998 - val_loss: 1.7298 - val_accuracy: 0.8166\n",
      "Epoch 46/60\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0039 - accuracy: 0.9999 - val_loss: 1.7422 - val_accuracy: 0.8171\n",
      "Epoch 47/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.7519 - val_accuracy: 0.8172\n",
      "Epoch 48/60\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.7625 - val_accuracy: 0.8167\n",
      "Epoch 49/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.7718 - val_accuracy: 0.8170\n",
      "Epoch 50/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.7816 - val_accuracy: 0.8172\n",
      "Epoch 51/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.7907 - val_accuracy: 0.8166\n",
      "Epoch 52/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.8021 - val_accuracy: 0.8170\n",
      "Epoch 53/60\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.8104 - val_accuracy: 0.8167\n",
      "Epoch 54/60\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.8188 - val_accuracy: 0.8161\n",
      "Epoch 55/60\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.8314 - val_accuracy: 0.8166\n",
      "Epoch 56/60\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.8432 - val_accuracy: 0.8161\n",
      "Epoch 57/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.8484 - val_accuracy: 0.8132\n",
      "Epoch 58/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0788 - accuracy: 0.9777 - val_loss: 1.7945 - val_accuracy: 0.8074\n",
      "Epoch 59/60\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0278 - accuracy: 0.9931 - val_loss: 1.7867 - val_accuracy: 0.8123\n",
      "Epoch 60/60\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 0.0075 - accuracy: 0.9992 - val_loss: 1.7950 - val_accuracy: 0.8167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7da4d0490940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_output,\n",
    "          batch_size=64,\n",
    "          epochs=60,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the encoder and decoder models for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T06:23:28.017097Z",
     "iopub.status.busy": "2023-06-30T06:23:28.016268Z",
     "iopub.status.idle": "2023-06-30T06:23:28.336438Z",
     "shell.execute_reply": "2023-06-30T06:23:28.335509Z",
     "shell.execute_reply.started": "2023-06-30T06:23:28.017060Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.save('encoder_model.h5')\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_embedding_inference = ger_embedding_layer(decoder_inputs)\n",
    "decoder_outputs_inference, state_h_inference, state_c_inference = decoder_lstm(decoder_embedding_inference,\n",
    "                                                                               initial_state=decoder_states_inputs)\n",
    "decoder_states_inference = [state_h_inference, state_c_inference]\n",
    "decoder_outputs_inference = decoder_dense(decoder_outputs_inference)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                      [decoder_outputs_inference] + decoder_states_inference)\n",
    "decoder_model.save('decoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T06:23:28.338111Z",
     "iopub.status.busy": "2023-06-30T06:23:28.337728Z",
     "iopub.status.idle": "2023-06-30T06:23:28.368656Z",
     "shell.execute_reply": "2023-06-30T06:23:28.367797Z",
     "shell.execute_reply.started": "2023-06-30T06:23:28.338079Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save eng_tokenizer\n",
    "eng_token_dict = {\"word_index\": eng_token.word_index}\n",
    "with open('eng_tokenizer.json', 'w') as f:\n",
    "    json.dump(eng_token_dict, f)\n",
    "\n",
    "# Save ger_tokenizer\n",
    "ger_token_dict = {\"word_index\": ger_token.word_index}\n",
    "with open('ger_tokenizer.json', 'w') as f:\n",
    "    json.dump(ger_token_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T06:23:28.371580Z",
     "iopub.status.busy": "2023-06-30T06:23:28.371228Z",
     "iopub.status.idle": "2023-06-30T06:23:28.392168Z",
     "shell.execute_reply": "2023-06-30T06:23:28.391378Z",
     "shell.execute_reply.started": "2023-06-30T06:23:28.371548Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save ger_token's index_word dictionary\n",
    "ger_token_dict = {\"index_word\": ger_token.index_word}\n",
    "with open('ger_token_index_word.json', 'w') as f:\n",
    "    json.dump(ger_token_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T06:23:28.393855Z",
     "iopub.status.busy": "2023-06-30T06:23:28.393299Z",
     "iopub.status.idle": "2023-06-30T06:23:28.402060Z",
     "shell.execute_reply": "2023-06-30T06:23:28.401145Z",
     "shell.execute_reply.started": "2023-06-30T06:23:28.393824Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to decode a new sentence\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = ger_token.word_index['<start>']\n",
    "    stop_condition=False\n",
    "    decoded_sentence=''\n",
    "    while not stop_condition:\n",
    "        output_tokens,h,c=decoder_model.predict([target_seq]+states_value,verbose=0)\n",
    "        sampled_token_index=np.argmax(output_tokens[0,-1,:])\n",
    "        sampled_word=ger_token.index_word[sampled_token_index]\n",
    "        if sampled_word != '<end>':\n",
    "            decoded_sentence += ' '+sampled_word\n",
    "\n",
    "        if (sampled_word == '<end>' or len(decoded_sentence.split()) > max_decoder_seq_length):\n",
    "            stop_condition=True\n",
    "\n",
    "        target_seq=np.zeros((1,1))\n",
    "        target_seq[0,0]=sampled_token_index\n",
    "\n",
    "        states_value=[h,c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T06:23:28.404212Z",
     "iopub.status.busy": "2023-06-30T06:23:28.403572Z",
     "iopub.status.idle": "2023-06-30T06:23:33.503831Z",
     "shell.execute_reply": "2023-06-30T06:23:33.502613Z",
     "shell.execute_reply.started": "2023-06-30T06:23:28.404181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 380ms/step\n",
      "Input sentence: hello\n",
      "Actual sentence: hallo\n",
      "Decoded sentence: hallo\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Input sentence: I won\n",
      "Actual sentence: ich habe gewonnen\n",
      "Decoded sentence: ich hab gewonnen\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Input sentence: Go Away\n",
      "Actual sentence: geh weg\n",
      "Decoded sentence: geh weg\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Input sentence: I gave up\n",
      "Actual sentence: ich gab auf\n",
      "Decoded sentence: ich habe gekotzt\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Input sentence: I am a man\n",
      "Actual sentence: ich bin ein Mann\n",
      "Decoded sentence: ich bin ein mann\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Input sentence: Tom really got a bad deal\n",
      "Actual sentence: tom hat wirklich ein schlechtes Geschäft gemacht\n",
      "Decoded sentence: tom hat wirklich schlecht\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Input sentence: he will go with us\n",
      "Actual sentence: er wird mit uns gehen\n",
      "Decoded sentence: er soll mit dem gehen\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Input sentence: what are you reading\n",
      "Actual sentence: was liest du?\n",
      "Decoded sentence: was liest mich\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Input sentence: hi\n",
      "Actual sentence: hallo\n",
      "Decoded sentence: hallo\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Input sentence: The reason does not matter\n",
      "Actual sentence: der Grund ist egal\n",
      "Decoded sentence: der wirtschaft geht es mir schlecht\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Input sentence: I love your cat\n",
      "Actual sentence: ich liebe deine Katze\n",
      "Decoded sentence: ich liebe deine katze\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Input sentence: go\n",
      "Actual sentence: geh\n",
      "Decoded sentence: geh\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Input sentence: get inside\n",
      "Actual sentence: Komm herein\n",
      "Decoded sentence: geh rein\n"
     ]
    }
   ],
   "source": [
    "listt=[\n",
    "    'hello',\n",
    "    'I won',\n",
    "    'Go Away',\n",
    "    'I gave up',\n",
    "    'I am a man',\n",
    "    'Tom really got a bad deal',\n",
    "    'he will go with us',\n",
    "    'what are you reading',\n",
    "    'hi',\n",
    "    'The reason does not matter',\n",
    "    'I love your cat',\n",
    "    'go',\n",
    "    'get inside'\n",
    "]\n",
    "actual=[\n",
    "    'hallo',\n",
    "    'ich habe gewonnen',\n",
    "    'geh weg',\n",
    "    'ich gab auf',\n",
    "    'ich bin ein Mann',\n",
    "    'tom hat wirklich ein schlechtes Geschäft gemacht',\n",
    "    'er wird mit uns gehen',\n",
    "    'was liest du?',\n",
    "    'hallo',\n",
    "    'der Grund ist egal',\n",
    "    'ich liebe deine Katze',\n",
    "    'geh',\n",
    "    'Komm herein'\n",
    "]\n",
    "for i in range(len(listt)):\n",
    "    new_english_sentence=listt[i]\n",
    "    new_english_sentence.lower().replace('[^\\w\\s]', '')\n",
    "    new_eng_integer_encoded=eng_token.texts_to_sequences([new_english_sentence])\n",
    "    new_encoder_input_data=pad_sequences(new_eng_integer_encoded,maxlen=max_encoder_seq_length,padding='post')\n",
    "    decoded_sentence=decode_sequence(new_encoder_input_data)\n",
    "    decoded_sentence=decoded_sentence.strip()\n",
    "    print('Input sentence:', new_english_sentence)\n",
    "    print('Actual sentence:', actual[i])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bleu Score of model Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T06:23:33.506335Z",
     "iopub.status.busy": "2023-06-30T06:23:33.505896Z",
     "iopub.status.idle": "2023-06-30T06:23:38.650109Z",
     "shell.execute_reply": "2023-06-30T06:23:38.649042Z",
     "shell.execute_reply.started": "2023-06-30T06:23:33.506292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "BLEU score: 0.3805647320367025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "references = []\n",
    "hypotheses = []\n",
    "for i in range(len(listt)):\n",
    "    input_seq=listt[i]\n",
    "    input_seq.lower().replace('[^\\w\\s]', '')\n",
    "    new_eng_integer_encoded=eng_token.texts_to_sequences([input_seq])\n",
    "    new_encoder_input_data=pad_sequences(new_eng_integer_encoded,maxlen=max_encoder_seq_length,padding='post')\n",
    "    actual_sentence = actual[i]\n",
    "    decoded_sentence = decode_sequence(new_encoder_input_data)\n",
    "    actual_sentence=actual_sentence.strip()\n",
    "    decoded_sentence=decoded_sentence.strip()\n",
    "    references.append([actual_sentence.split()])\n",
    "    hypotheses.append(decoded_sentence.split())\n",
    "bleu_score = corpus_bleu(references, hypotheses)\n",
    "print('BLEU score:', bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
